{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Basics\n",
    "\n",
    "Learn to prepare data for machine learning.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Feature Scaling](#scaling)\n",
    "2. [Encoding Categorical Variables](#encoding)\n",
    "3. [Handling Missing Values](#missing)\n",
    "4. [Feature Selection](#selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling <a id='scaling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = np.array([[1, 100], [2, 200], [3, 300], [4, 400], [5, 500]])\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "\n",
    "# Standardization (Z-score)\n",
    "scaler = StandardScaler()\n",
    "standardized = scaler.fit_transform(data)\n",
    "print(\"\\nStandardized:\")\n",
    "print(standardized)\n",
    "\n",
    "# Min-Max Scaling\n",
    "minmax = MinMaxScaler()\n",
    "normalized = minmax.fit_transform(data)\n",
    "print(\"\\nMin-Max scaled:\")\n",
    "print(normalized)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].scatter(data[:, 0], data[:, 1])\n",
    "axes[0].set_title('Original')\n",
    "axes[1].scatter(standardized[:, 0], standardized[:, 1])\n",
    "axes[1].set_title('Standardized')\n",
    "axes[2].scatter(normalized[:, 0], normalized[:, 1])\n",
    "axes[2].set_title('Normalized')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables <a id='encoding'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue'],\n",
    "    'Size': ['S', 'M', 'L', 'M', 'S'],\n",
    "    'Price': [10, 20, 30, 15, 25]\n",
    "})\n",
    "print(\"Original data:\")\n",
    "print(df)\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df['Color_Label'] = le.fit_transform(df['Color'])\n",
    "print(\"\\nWith Label Encoding:\")\n",
    "print(df)\n",
    "\n",
    "# One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df[['Color', 'Size']], drop_first=True)\n",
    "print(\"\\nOne-Hot Encoded:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values <a id='missing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, 30, 40, 50],\n",
    "    'C': ['x', 'y', 'z', np.nan, 'y']\n",
    "})\n",
    "print(\"Data with missing values:\")\n",
    "print(df)\n",
    "print(\"\\nMissing value counts:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Impute with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = df.copy()\n",
    "df_imputed[['A', 'B']] = imputer.fit_transform(df[['A', 'B']])\n",
    "print(\"\\nAfter mean imputation:\")\n",
    "print(df_imputed)\n",
    "\n",
    "# Impute categorical with most frequent\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "df_imputed['C'] = imputer_cat.fit_transform(df[['C']])\n",
    "print(\"\\nAfter categorical imputation:\")\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection <a id='selection'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Target depends on first two features\n",
    "\n",
    "print(\"Original features shape:\", X.shape)\n",
    "\n",
    "# Select top k features\n",
    "selector = SelectKBest(chi2, k=3)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected features shape:\", X_new.shape)\n",
    "print(\"Feature scores:\", selector.scores_)\n",
    "print(\"Selected feature indices:\", selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Covered:\n",
    "- Feature scaling (standardization, normalization)\n",
    "- Encoding categorical variables\n",
    "- Handling missing values\n",
    "- Feature selection\n",
    "\n",
    "Next: **Machine Learning** algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
